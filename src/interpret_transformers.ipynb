{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the work involves trying to observe copying and shift-1 behavior, and induction heads as described here: https://transformer-circuits.pub/2021/framework/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import transformers\n",
    "import torch\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import minigpt\n",
    "\n",
    "\n",
    "model = mingpt.get_minigpt(\"model.pt\")\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer._add_tokens([\"[BEGIN]\", \"[END]\"])\n",
    "tokenizer.pad_token = \"[END]\"\n",
    "tokenizer.eos_token = \"[END]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the model. It is a mini GPT with 2 transformer blocks, without non-linearities to make it simpler to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a particular head, we try computing the attention terms, separated into token-token, token-position, position-token, position-position terms (in order). We observe that:\n",
    "\n",
    "* position-position terms have the largest magnitude and seem to be the most important\n",
    "* position-position: we see that a position pays attention to keys from previous index (from the diagonal i+1, i)\n",
    "* token-token: we see that a token pays quite a bit of attention to itself (the diagonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_terms(sentence, layer, head):\n",
    "    \"\"\"Input of shape: batch_size seq_len\"\"\"\n",
    "    token_embedding = model.token_embedding(sentence)\n",
    "    \n",
    "    # Get the positions, then index into the position embedding\n",
    "    seq_len = sentence.shape[1]\n",
    "    pos_inds = torch.arange(seq_len).unsqueeze(0)\n",
    "    pos_embedding = model.pos_embedding(pos_inds)\n",
    "        \n",
    "    aggregated = 0\n",
    "    \n",
    "    for l in range(layer):\n",
    "        token_embedding += model.blocks[l](token_embedding, model.pos_embedding)\n",
    "    \n",
    "    for query_embedding in (token_embedding, pos_embedding):\n",
    "        for key_embedding in (token_embedding, pos_embedding):\n",
    "            query = model.blocks[layer].project_qkv(query_embedding)[:, :, :256]\n",
    "            key = model.blocks[layer].project_qkv(key_embedding)[:, :, 256:512]\n",
    "\n",
    "            # reshape\n",
    "            num_heads = 8\n",
    "            query = einops.rearrange(query, 'batch seq_len (num_heads head_size) -> batch num_heads seq_len head_size', num_heads=num_heads)\n",
    "            key = einops.rearrange(key, 'batch seq_len (num_heads head_size) -> batch num_heads seq_len head_size', num_heads=num_heads)\n",
    "            \n",
    "            # Extract the desired Q and K matrices\n",
    "            # Each matrix: seq_len, hidden size\n",
    "            result = torch.einsum('b h q c, b h k c -> b h q k', query, key)\n",
    "            # Want to get embedding1 in Q space, and embedding2 in K space\n",
    "            \n",
    "            result = result[0][head] # head 0\n",
    "            result = torch.tril(result)\n",
    "\n",
    "            print('shape:', result.shape)\n",
    "            print('mean:', torch.mean(result).detach())\n",
    "            # Compute variance across token pairs\n",
    "            print('var:', torch.var(result, unbiased=False).detach().item())\n",
    "            \n",
    "            for y in range(10):\n",
    "                l = []\n",
    "                for x in range(10):\n",
    "                    l.append(f'{result[y, x].item():8.2}')\n",
    "                print(' '.join(l))\n",
    "            print(plt.imshow(torch.nn.Softmax(dim=-1)(result)[:10,:10].detach()))\n",
    "            plt.show()\n",
    "            \n",
    "            print()\n",
    "            aggregated += result\n",
    "    print('aggregated')\n",
    "    print('shape:', aggregated.shape)\n",
    "    print('mean:', torch.mean(aggregated).detach())\n",
    "    # Compute variance across token pairs\n",
    "    print('var:', torch.var(aggregated, unbiased=False).detach().item())\n",
    "    print(plt.imshow(torch.nn.Softmax(dim=-1)(aggregated)[:10,:10].detach()))\n",
    "    plt.show()\n",
    "    \n",
    "    # TODO: Compute variance across sentences (i.e. across the batch)\n",
    "    \n",
    "sentences = ['''[BEGIN] We dog eating ice cream. We dog in the park. We dog playing basketball. We dog throwing a baseball. We dog learning about machine learning. We dog trying to debug. We dog confused. We dog playing with the visualizer.''']\n",
    "tokens = tokenizer(sentences)['input_ids']\n",
    "print(f\"{tokenizer.decode([tokens[0][7], tokens[0][2]])=}\")\n",
    "compute_attention_terms(torch.tensor(tokens, dtype=torch.long))\n",
    "\n",
    "# Are the position most interested in those that are at index one more than the key?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempt to observe virtual heads, which are possible due to the residual stream in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def virtual_head(tokens, layer0_head_idx, layer1_head_idx, model):\n",
    "    # embedding.shape = [batch, seq_len, hidden_size]\n",
    "    # precompute the attention matrices\n",
    "    # A = softmax(((emb * Q)^T * K * emb) / sqrt(d))\n",
    "    \n",
    "    def get_attention_and_ov_matrix(token_emb, layer, head_idx):\n",
    "        \n",
    "        pos_ids = torch.arange(token_emb.shape[1]).unsqueeze(0)\n",
    "        emb = model.pos_embedding(pos_ids) + token_emb\n",
    "        query = model.blocks[layer].project_qkv(emb)[:, :, :256]\n",
    "        key = model.blocks[layer].project_qkv(emb)[:, :, 256:512]\n",
    "        # val = model.blocks[layer].project_qkv(emb)[:, :, 512:]\n",
    "        \n",
    "        # reshape\n",
    "        num_heads = 8\n",
    "        query = einops.rearrange(query, 'batch seq_len (num_heads head_size) -> batch num_heads seq_len head_size', num_heads=num_heads)\n",
    "        key = einops.rearrange(key, 'batch seq_len (num_heads head_size) -> batch num_heads seq_len head_size', num_heads=num_heads)\n",
    "        # val = einops.rearrange(val, 'batch seq_len (num_heads head_size) -> batch num_heads seq_len head_size', num_heads=num_heads)\n",
    "\n",
    "        attn = torch.einsum('b h q c, b h k c -> b h q k', query, key)\n",
    "        # Want to get embedding1 in Q space, and embedding2 in K space\n",
    "\n",
    "        attn = attn[0][head_idx] # batch 0\n",
    "\n",
    "        seq_len = emb.shape[1] # (batch, seq, hidden_size)\n",
    "        mask = torch.tensor(torch.triu(torch.ones(seq_len, seq_len), diagonal=1), dtype=torch.bool)\n",
    "        attn[mask] = -1000000\n",
    "        attn /= math.sqrt(query.shape[-1])\n",
    "        # For each query, we have a probability distribution over keys\n",
    "        attn = torch.nn.Softmax(dim=-1)(attn)\n",
    "        \n",
    "        ## get ov ###\n",
    "        # project_qkv.weight should be a 768x768 matrix (hidden_size=768)\n",
    "        print(model.blocks[layer].project_qkv.weight.shape)\n",
    "        # ... V\n",
    "        val_mat = model.blocks[layer].project_qkv.weight[512:,:]\n",
    "        print(val_mat.shape)\n",
    "        val_mat = einops.rearrange(val_mat, '(num_heads head_size) hidden_size -> num_heads hidden_size head_size', num_heads=num_heads)\n",
    "        \n",
    "        print(val_mat.shape)\n",
    "        val_mat = val_mat[head_idx]  # now shape = (hidden_size, head_size)\n",
    "        print(val_mat.shape)\n",
    "        head_size = 256//num_heads\n",
    "        output = model.blocks[layer].project_output.weight[:, head_size*head_idx:head_size*(head_idx+1)]\n",
    "        output = einops.rearrange(output, 'hidden_size head_size -> head_size hidden_size')\n",
    "        # val_head: hidden_size head_size\n",
    "        # output: head_size, hidden_size??\n",
    "        # softmax(...)V W_O\n",
    "        ov = val_mat @ output\n",
    "        \n",
    "        # OV emb\n",
    "        return attn, ov\n",
    "    \n",
    "    ##### lAYER 0 ######\n",
    "    token_embedding = model.token_embedding(tokens)\n",
    "    attn0, ov0 = get_attention_and_ov_matrix(token_embedding, 0, layer0_head_idx)\n",
    "    # (seq_len seq_len), (seq_len, hidden_size)\n",
    "    \n",
    "    ##### LAYER 1 ######\n",
    "    emb = token_embedding + model.blocks[0](token_embedding, model.pos_embedding)\n",
    "    attn1, ov1 = get_attention_and_ov_matrix(emb, 1, layer1_head_idx)\n",
    "    plt.imshow(attn0.detach())\n",
    "    plt.show()\n",
    "    plt.imshow(attn1.detach())\n",
    "\n",
    "    # token_embedding: batch_size, seq_len, embedding_size (probs hidden_size?)\n",
    "    # attn0: seq_len, seq_len\n",
    "    # ov0: hidden_size, hidden_size=c\n",
    "    emb1 = torch.einsum('b k c, q k, c d -> b q d', token_embedding, attn0, ov0)\n",
    "    emb2 = torch.einsum('b k c, q k, c d -> b q d ', emb1+token_embedding, attn1, ov1)\n",
    "    # We want: seq_len, hidden_size\n",
    "    logits = torch.einsum('bnl, vl -> bnv', emb2, model.token_embedding.weight)\n",
    "    # multiply by the relevant model_weights, and the embedding to get the vocab logits\n",
    "    _, tokens = torch.topk(logits, k=10, dim=-1)\n",
    "    return [(i, x) for i, x in enumerate(tokenizer.batch_decode(tokens[0]))]\n",
    "    # We want: seq_len, vocab_size for each pair of heads\n",
    "print(tokenizer.decode(tokens[0]))\n",
    "virtual_head(torch.tensor(tokens, dtype=torch.long), 0, 4, model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
